{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to datasets\n",
    "PATH_TO_DATASETS_DIRECTORY = './'\n",
    "datasets = {\n",
    "    'reddit': Reddit(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Reddit'),\n",
    "    'cora' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Cora/', name='Cora'),\n",
    "    'citeseer' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/CiteSeer/', name='CiteSeer'),\n",
    "    'pubmed' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/PubMed/', name='PubMed'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain dataset\n",
    "DATASET = 'cora'\n",
    "PREDICTION = 'node'\n",
    "dataset = datasets[DATASET]\n",
    "data = dataset[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "predictions = {\n",
    "    'node' : dataset.num_classes,\n",
    "    'link' : 2,\n",
    "    'triad' : 4,\n",
    "}\n",
    "\n",
    "# Get train, val, test data\n",
    "data.train_mask = ~data.val_mask * ~data.test_mask\n",
    "\n",
    "adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "edges = data.edge_index.t()\n",
    "adj_mat[edges[:,0], edges[:,1]] = 1\n",
    "\n",
    "# Non-overlapping induced subgraphs\n",
    "adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge/non-edge corruption\n",
    "def corrupt_adj(adj_mat, task, percent=2):\n",
    "    \"\"\" Returns the corrupted version of the adjacency matrix \"\"\"\n",
    "    if task == 'link':\n",
    "        edges = adj_mat.triu().nonzero()\n",
    "        num_edges = edges.shape[0]\n",
    "        num_to_corrupt = int(percent/100.0 * num_edges)\n",
    "        random_corruption = np.random.randint(num_edges, size=num_to_corrupt)\n",
    "        adj_mat_corrupted = adj_mat.clone()\n",
    "        false_edges, false_non_edges = [], []\n",
    "        #Edge Corruption\n",
    "        for ed in edges[random_corruption]:\n",
    "            adj_mat_corrupted[ed[0], ed[1]] = 0\n",
    "            adj_mat_corrupted[ed[1], ed[0]] = 0\n",
    "            false_non_edges.append(ed.tolist())\n",
    "        #Non Edge Corruption\n",
    "        random_non_edge_corruption = list(np.random.randint(adj_mat.shape[0], size = 6*num_to_corrupt))\n",
    "        non_edge_to_corrupt = []\n",
    "        for k in range(len(random_non_edge_corruption)-1):\n",
    "            to_check = [random_non_edge_corruption[k], random_non_edge_corruption[k+1]]\n",
    "            if to_check not in edges.tolist():\n",
    "                non_edge_to_corrupt.append(to_check)\n",
    "            if len(non_edge_to_corrupt) == num_to_corrupt:\n",
    "                break\n",
    "        non_edge_to_corrupt = torch.Tensor(non_edge_to_corrupt).type(torch.int16)\n",
    "        for n_ed in non_edge_to_corrupt:\n",
    "            adj_mat_corrupted[n_ed[0], n_ed[1]] = 1\n",
    "            adj_mat_corrupted[n_ed[1], n_ed[0]] = 1\n",
    "            false_edges.append(n_ed.tolist())\n",
    "    return adj_mat_corrupted, false_edges, false_non_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised learning network\n",
    "num_neurons = 256\n",
    "input_rep = num_neurons + data.num_features\n",
    "\n",
    "class StructMLP(nn.Module):\n",
    "    \"\"\"\n",
    "        Compute an estimate of the expected value of a function of node embeddings\n",
    "        Permutation Invariant Function - Deepsets - Zaheer, et al.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_set_size=1):\n",
    "        super(StructMLP, self).__init__()\n",
    "\n",
    "        self.node_set_size = node_set_size\n",
    "        #Deepsets MLP\n",
    "\n",
    "        self.ds_layer_1 = nn.Linear(input_rep*node_set_size, num_neurons)\n",
    "        self.ds_layer_2 = nn.Linear(num_neurons, num_neurons)\n",
    "\n",
    "        #One Hidden Layer\n",
    "        self.layer1 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.layer2 = nn.Linear(num_neurons, predictions[PREDICTION])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        #Deepsets initially on each of the samples\n",
    "        num_nodes = input_tensor.shape[1]\n",
    "        comb_tensor = torch.LongTensor(list(combinations(range(num_nodes), self.node_set_size)))\n",
    "        sum_tensor = torch.zeros(comb_tensor.shape[0], num_neurons).to(device)\n",
    "\n",
    "        for i in range(input_tensor.shape[0]):\n",
    "            #Process the input tensor to form n choose k combinations and create a zero tensor\n",
    "            set_init_rep = input_tensor[i][comb_tensor].view(comb_tensor.shape[0],-1)\n",
    "\n",
    "            x = self.ds_layer_1(set_init_rep)\n",
    "            x = self.relu(x)\n",
    "            x = self.ds_layer_2(x)\n",
    "            sum_tensor += x\n",
    "\n",
    "        x = sum_tensor / input_tensor.shape[0]\n",
    "\n",
    "        #One Hidden Layer for predictor\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, input_tensor, target):\n",
    "        pred = self.forward(input_tensor)\n",
    "        return F.cross_entropy(pred, target)\n",
    "\n",
    "if PREDICTION == 'node':\n",
    "    node_set_size = 1\n",
    "elif PREDICTION == 'link':\n",
    "    node_set_size = 2\n",
    "else:\n",
    "    node_set_size = 3\n",
    "\n",
    "if PREDICTION == 'node':\n",
    "    target_train = data.y[data.train_mask].type(torch.long)\n",
    "    target_val = data.y[data.val_mask].type(torch.long)\n",
    "    target_test = data.y[data.test_mask].type(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleZ(adj, ns, ni):\n",
    "    numbers = list(np.random.randint(500, size=ns))\n",
    "    hidden_samples = []\n",
    "    for number in numbers :\n",
    "        svd = TruncatedSVD(n_components=256, n_iter=ni, random_state=number)\n",
    "        u = svd.fit_transform(adj)\n",
    "        hidden_samples.append(torch.Tensor(u).to(device))\n",
    "    return hidden_samples\n",
    "\n",
    "# NOTE: Remember to set num_neurons as dim(hidden_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 sample(s) of 10 iteration(s) of T-SVD at 1% corruption\n",
      " - Test 1\n",
      " -- MF1, WF1, BVL: 0.686, 0.662, 0.9834\n",
      " -- Finished in:   120.448 s\n",
      " - Test 2\n",
      " -- MF1, WF1, BVL: 0.663, 0.628, 1.0623\n",
      " -- Finished in:   124.228 s\n",
      " - Test 3\n",
      " -- MF1, WF1, BVL: 0.670, 0.634, 1.0224\n",
      " -- Finished in:   143.160 s\n",
      " - Test 4\n",
      " -- MF1, WF1, BVL: 0.672, 0.636, 1.0344\n",
      " -- Finished in:   117.874 s\n",
      " - Test 5\n",
      " -- MF1, WF1, BVL: 0.672, 0.632, 1.0826\n",
      " -- Finished in:   118.623 s\n",
      " - Test 6\n",
      " -- MF1, WF1, BVL: 0.676, 0.653, 1.0801\n",
      " -- Finished in:   119.749 s\n",
      " - Test 7\n",
      " -- MF1, WF1, BVL: 0.663, 0.608, 1.0547\n",
      " -- Finished in:   119.625 s\n",
      " - Test 8\n",
      " -- MF1, WF1, BVL: 0.648, 0.594, 1.0862\n",
      " -- Finished in:   120.307 s\n",
      " - Test 9\n",
      " -- MF1, WF1, BVL: 0.663, 0.633, 1.0604\n",
      " -- Finished in:   120.228 s\n",
      " - Test 10\n",
      " -- MF1, WF1, BVL: 0.659, 0.617, 1.0269\n",
      " -- Finished in:   120.319 s\n",
      " - Test 11\n",
      " -- MF1, WF1, BVL: 0.638, 0.585, 1.1132\n",
      " -- Finished in:   120.508 s\n",
      " - Test 12\n",
      " -- MF1, WF1, BVL: 0.676, 0.643, 1.0316\n",
      " -- Finished in:   120.735 s\n",
      "\n",
      " - 5, 10 tests complete\n",
      " - Micro F1 Score mean[std]:    0.6655[0.0125]\n",
      " - Weighted F1 Score mean[std]: 0.6271[0.0217]\n",
      "\n",
      "5 sample(s) of 10 iteration(s) of T-SVD at 2% corruption\n",
      " - Test 1\n",
      " -- MF1, WF1, BVL: 0.676, 0.655, 1.0649\n",
      " -- Finished in:   120.943 s\n",
      " - Test 2\n",
      " -- MF1, WF1, BVL: 0.659, 0.616, 1.0170\n",
      " -- Finished in:   120.848 s\n",
      " - Test 3\n",
      " -- MF1, WF1, BVL: 0.667, 0.639, 1.0163\n",
      " -- Finished in:   120.863 s\n",
      " - Test 4\n",
      " -- MF1, WF1, BVL: 0.650, 0.601, 1.0676\n",
      " -- Finished in:   121.384 s\n",
      " - Test 5\n",
      " -- MF1, WF1, BVL: 0.658, 0.617, 1.0567\n",
      " -- Finished in:   121.356 s\n",
      " - Test 6\n",
      " -- MF1, WF1, BVL: 0.661, 0.623, 1.0012\n",
      " -- Finished in:   122.537 s\n",
      " - Test 7\n",
      " -- MF1, WF1, BVL: 0.659, 0.608, 1.0440\n",
      " -- Finished in:   122.308 s\n",
      " - Test 8\n",
      " -- MF1, WF1, BVL: 0.656, 0.607, 1.0487\n",
      " -- Finished in:   126.713 s\n",
      " - Test 9\n",
      " -- MF1, WF1, BVL: 0.650, 0.608, 1.0311\n",
      " -- Finished in:   133.688 s\n",
      " - Test 10\n",
      " -- MF1, WF1, BVL: 0.659, 0.618, 1.0601\n",
      " -- Finished in:   121.838 s\n",
      " - Test 11\n",
      " -- MF1, WF1, BVL: 0.652, 0.606, 1.0634\n",
      " -- Finished in:   123.785 s\n",
      " - Test 12\n",
      " -- MF1, WF1, BVL: 0.647, 0.591, 1.0967\n",
      " -- Finished in:   119.283 s\n",
      "\n",
      " - 5, 10 tests complete\n",
      " - Micro F1 Score mean[std]:    0.6578[0.0077]\n",
      " - Weighted F1 Score mean[std]: 0.6159[0.0166]\n",
      "\n",
      "5 sample(s) of 10 iteration(s) of T-SVD at 5% corruption\n",
      " - Test 1\n",
      " -- MF1, WF1, BVL: 0.642, 0.591, 1.0603\n",
      " -- Finished in:   118.763 s\n",
      " - Test 2\n",
      " -- MF1, WF1, BVL: 0.659, 0.613, 1.0369\n",
      " -- Finished in:   116.878 s\n",
      " - Test 3\n",
      " -- MF1, WF1, BVL: 0.668, 0.653, 0.9901\n",
      " -- Finished in:   117.021 s\n",
      " - Test 4\n",
      " -- MF1, WF1, BVL: 0.687, 0.661, 0.9938\n",
      " -- Finished in:   116.825 s\n",
      " - Test 5\n",
      " -- MF1, WF1, BVL: 0.656, 0.617, 1.0681\n",
      " -- Finished in:   117.391 s\n",
      " - Test 6\n",
      " -- MF1, WF1, BVL: 0.694, 0.677, 1.0031\n",
      " -- Finished in:   117.450 s\n",
      " - Test 7\n",
      " -- MF1, WF1, BVL: 0.658, 0.601, 1.0823\n",
      " -- Finished in:   117.575 s\n",
      " - Test 8\n",
      " -- MF1, WF1, BVL: 0.653, 0.604, 1.0859\n",
      " -- Finished in:   117.574 s\n",
      " - Test 9\n",
      " -- MF1, WF1, BVL: 0.660, 0.627, 0.9857\n",
      " -- Finished in:   117.818 s\n",
      " - Test 10\n",
      " -- MF1, WF1, BVL: 0.654, 0.607, 1.0612\n",
      " -- Finished in:   117.706 s\n",
      " - Test 11\n",
      " -- MF1, WF1, BVL: 0.639, 0.583, 1.0913\n",
      " -- Finished in:   118.279 s\n",
      " - Test 12\n",
      " -- MF1, WF1, BVL: 0.655, 0.604, 1.0877\n",
      " -- Finished in:   117.774 s\n",
      "\n",
      " - 5, 10 tests complete\n",
      " - Micro F1 Score mean[std]:    0.6604[0.0154]\n",
      " - Weighted F1 Score mean[std]: 0.6198[0.0280]\n"
     ]
    }
   ],
   "source": [
    "# Here's the magic:\n",
    "# Parameters for t-SVD sampling\n",
    "RUN_COUNT = 12\n",
    "NUM_SAMPLES_LIST = [5]\n",
    "NUM_ITERS_LIST   = [10]\n",
    "PERCENT_LIST     = [1, 2, 5] \n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "for NUM_SAMPLES, NUM_ITERS, PERCENT in itertools.product(NUM_SAMPLES_LIST, NUM_ITERS_LIST, PERCENT_LIST):\n",
    "    \n",
    "    print(\"\\n{} sample(s) of {} iteration(s) of T-SVD at {}% corruption\".format(NUM_SAMPLES, NUM_ITERS, PERCENT))\n",
    "    results = []\n",
    "    \n",
    "    for run in range(RUN_COUNT):\n",
    "        print(\" - Test {}\".format(run+1))\n",
    "        start = time.time()\n",
    "\n",
    "        # Corrupt edges\n",
    "        adj_train_corrupted, train_false_edges, train_false_non_edges = corrupt_adj(adj_train, 'link', percent=PERCENT)\n",
    "        adj_val_corrupted, val_false_edges, val_false_non_edges = corrupt_adj(adj_validation, 'link', percent=PERCENT)\n",
    "        adj_test_corrupted, test_false_edges, test_false_non_edges  = corrupt_adj(adj_test, 'link', percent=PERCENT)\n",
    "        \n",
    "        # Create a new model\n",
    "        mlp = StructMLP(node_set_size).to(device)\n",
    "        mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "        mlp_model = 'best_mlp_model.model'\n",
    "\n",
    "        # Training with current sampling params\n",
    "#         epochs = 50\n",
    "        epochs = 50\n",
    "        validation_loss = 10000.0\n",
    "        for num_epoch in range(epochs):\n",
    "#             print(num_epoch)\n",
    "            mlp_optimizer.zero_grad()\n",
    "            target = target_train.to(device)\n",
    "#             numbers = list(np.random.randint(500, size=NUM_SAMPLES))\n",
    "            hidden_samples_train = sampleZ(adj_train_corrupted, NUM_SAMPLES, NUM_ITERS)\n",
    "#             for number in numbers :\n",
    "#                 svd = TruncatedSVD(n_components=256, n_iter=NUM_ITERS, random_state=number)\n",
    "#                 u_train = svd.fit_transform(adj_train_corrupted)\n",
    "#                 hidden_samples_train.append(torch.Tensor(u_train).to(device))\n",
    "            for i in range(NUM_SAMPLES):\n",
    "                hidden_samples_train[i] = torch.cat((hidden_samples_train[i].to(device), data.x[data.train_mask].to(device)),1)\n",
    "            input_ = torch.stack(hidden_samples_train)\n",
    "            input_ = input_.detach()\n",
    "            loss = mlp.compute_loss(input_, target)\n",
    "            #print(\"Training Loss: \", loss.item())\n",
    "            with torch.no_grad():\n",
    "                #Do Validation and check if validation loss has gone down\n",
    "#                 numbers = list(np.random.randint(500, size=NUM_SAMPLES))\n",
    "                hidden_samples_validation = sampleZ(adj_val_corrupted, NUM_SAMPLES, NUM_ITERS)\n",
    "#                 for number in numbers :\n",
    "#                     svd = TruncatedSVD(n_components=256, n_iter=NUM_ITERS, random_state=number)\n",
    "#                     u_validation = svd.fit_transform(adj_val_corrupted)\n",
    "#                     hidden_samples_validation.append(torch.Tensor(u_validation).to(device))\n",
    "                for i in range(NUM_SAMPLES):\n",
    "                    hidden_samples_validation[i] = torch.cat((hidden_samples_validation[i].to(device), data.x[data.val_mask].to(device)),1)\n",
    "                input_val = torch.stack(hidden_samples_validation)\n",
    "                input_val = input_val.detach()\n",
    "                compute_val_loss = mlp.compute_loss(input_val, target_val.to(device))\n",
    "                if compute_val_loss < validation_loss:\n",
    "                    validation_loss = compute_val_loss\n",
    "                    #print(\"Validation Loss: \", validation_loss)\n",
    "                    #Save Model\n",
    "                    torch.save(mlp.state_dict(), mlp_model)\n",
    "            loss.backward()\n",
    "            mlp_optimizer.step()\n",
    "        \n",
    "        #print(\" -- Best val loss:     {}\".format(validation_loss))\n",
    "        \n",
    "        # Load best model\n",
    "        mlp = StructMLP(node_set_size).to(device)\n",
    "        mlp.load_state_dict(torch.load(mlp_model))\n",
    "        \n",
    "        # Forward pass on test set\n",
    "#         numbers = list(np.random.randint(500, size=NUM_SAMPLES))\n",
    "        hidden_samples_test = sampleZ(adj_test_corrupted, NUM_SAMPLES, NUM_ITERS)\n",
    "#         for number in numbers :\n",
    "#             svd = TruncatedSVD(n_components=256, n_iter=NUM_ITERS, random_state=number)\n",
    "#             u_test = svd.fit_transform(adj_test_corrupted)\n",
    "#             hidden_samples_test.append(torch.Tensor(u_test).to(device))\n",
    "        for i in range(NUM_SAMPLES):\n",
    "            hidden_samples_test[i] = torch.cat((hidden_samples_test[i].to(device), data.x[data.test_mask].to(device)),1)\n",
    "        t_test = target_test.to(\"cpu\").numpy()\n",
    "        input_test = torch.stack(hidden_samples_test)\n",
    "        input_test = input_test.detach()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_pred = mlp.forward(input_test)\n",
    "            pred = F.log_softmax(test_pred, dim=1)\n",
    "\n",
    "        pred = pred.detach().to(\"cpu\").numpy()\n",
    "        pred = np.argmax(pred, axis=1)\n",
    "        \n",
    "        # Obtain results for run\n",
    "        mf1 = f1_score(t_test, pred, average='micro')\n",
    "        wf1 = f1_score(t_test, pred, average='weighted')\n",
    "        results.append([mf1, wf1])\n",
    "        #print(\" -- Micro F1 Score:    {}\".format(mf1))\n",
    "        #print(\" -- Weighted F1 Score: {}\".format(wf1))\n",
    "        print(\" -- MF1, WF1, BVL: {:.3f}, {:.3f}, {:.4f}\".format(mf1, wf1, validation_loss))\n",
    "        print(\" -- Finished in:   {:.3f} s\".format(time.time() - start))\n",
    "    \n",
    "    results = np.array(results)\n",
    "    m, s = np.mean(results, axis=0), np.std(results, axis=0)\n",
    "    print(\"\\n - {}, {} tests complete\".format(NUM_SAMPLES, NUM_ITERS))\n",
    "    print(\" - Micro F1 Score mean[std]:    {:.4f}[{:.4f}]\".format(m[0], s[0]))\n",
    "    print(\" - Weighted F1 Score mean[std]: {:.4f}[{:.4f}]\".format(m[1], s[1]))\n",
    "    result_dict[str(NUM_SAMPLES) + \",\" + str(NUM_ITERS) + \",\" + str(PERCENT)] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = {}\n",
    "for k, v in result_dict.items():\n",
    "    result_data[k] = v.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(DATASET + '__svd_5_10_perc.json', 'w') as fp:\n",
    "    json.dump(result_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
