{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MC-SVD Procedure - Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import init\n",
    "from random import shuffle, randint\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Reddit, PPI, Planetoid\n",
    "from itertools import combinations, combinations_with_replacement\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset, the type of prediction and the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cora'\n",
    "PREDICTION = 'link'\n",
    "RUN_COUNT = 1\n",
    "NUM_SAMPLES = 1\n",
    "PATH_TO_DATASETS_DIRECTORY = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'reddit': Reddit(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Reddit'),\n",
    "    'cora' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/Cora/', name='Cora'),\n",
    "    'citeseer' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/CiteSeer/', name='CiteSeer'),\n",
    "    'pubmed' : Planetoid(root=PATH_TO_DATASETS_DIRECTORY + '/datasets/PubMed/', name='PubMed'),\n",
    "}\n",
    "dataset = datasets[DATASET]\n",
    "data = dataset[0]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'node' : dataset.num_classes,\n",
    "    'link' : 2,\n",
    "    'triad' : 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Dataset Characteristics\n",
      "Name:  cora\n",
      "Total Number of Nodes:  2708\n",
      "Total Number of Training Nodes:  140\n",
      "Total Number of Val Nodes:  500\n",
      "Total Number of Test Nodes:  1000\n",
      "Num Node Features:  1433\n",
      "Num Node Classes:  7\n",
      "Number of Edges:  10556\n",
      "Number of Samples for structural:  1\n",
      "Prediction Type:  link\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing Dataset Characteristics\")\n",
    "print(\"Name: \", DATASET)\n",
    "print(\"Total Number of Nodes: \", data.num_nodes)\n",
    "print(\"Total Number of Training Nodes: \", data.train_mask.sum().item())\n",
    "print(\"Total Number of Val Nodes: \", data.val_mask.sum().item())\n",
    "print(\"Total Number of Test Nodes: \", data.test_mask.sum().item())\n",
    "print(\"Num Node Features: \", data.num_features)\n",
    "print(\"Num Node Classes: \", dataset.num_classes)\n",
    "print(\"Number of Edges: \", data.edge_index.shape[1])\n",
    "print(\"Number of Samples for structural: \", NUM_SAMPLES)\n",
    "print(\"Prediction Type: \", PREDICTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.train_mask = 1 - data.val_mask - data.test_mask\n",
    "data.train_mask = ~data.val_mask * ~data.test_mask\n",
    "\n",
    "adj_mat = torch.zeros((data.num_nodes,data.num_nodes))\n",
    "edges = data.edge_index.t()\n",
    "adj_mat[edges[:,0], edges[:,1]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the non-overlapping induced subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train = adj_mat[data.train_mask].t()[data.train_mask].t()\n",
    "adj_validation = adj_mat[data.val_mask].t()[data.val_mask].t()\n",
    "adj_test = adj_mat[data.test_mask].t()[data.test_mask].t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrupt a small fraction of the edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrupt_adj(adj_mat, task, percent=2):\n",
    "    \"\"\" Returns the corrupted version of the adjacency matrix \"\"\"\n",
    "    if task == 'link':\n",
    "        edges = adj_mat.triu().nonzero()\n",
    "        num_edges = edges.shape[0]\n",
    "        num_to_corrupt = int(percent/100.0 * num_edges)\n",
    "        random_corruption = np.random.randint(num_edges, size=num_to_corrupt)\n",
    "        adj_mat_corrupted = adj_mat.clone()\n",
    "        false_edges, false_non_edges = [], []\n",
    "        #Edge Corruption\n",
    "        for ed in edges[random_corruption]:\n",
    "            adj_mat_corrupted[ed[0], ed[1]] = 0\n",
    "            adj_mat_corrupted[ed[1], ed[0]] = 0\n",
    "            false_non_edges.append(ed.tolist())\n",
    "        #Non Edge Corruption\n",
    "        random_non_edge_corruption = list(np.random.randint(adj_mat.shape[0], size = 6*num_to_corrupt))\n",
    "        non_edge_to_corrupt = []\n",
    "        for k in range(len(random_non_edge_corruption)-1):\n",
    "            to_check = [random_non_edge_corruption[k], random_non_edge_corruption[k+1]]\n",
    "            if to_check not in edges.tolist():\n",
    "                non_edge_to_corrupt.append(to_check)\n",
    "            if len(non_edge_to_corrupt) == num_to_corrupt:\n",
    "                break\n",
    "        non_edge_to_corrupt = torch.Tensor(non_edge_to_corrupt).type(torch.int16)\n",
    "        for n_ed in non_edge_to_corrupt:\n",
    "            adj_mat_corrupted[n_ed[0], n_ed[1]] = 1\n",
    "            adj_mat_corrupted[n_ed[1], n_ed[0]] = 1\n",
    "            false_edges.append(n_ed.tolist())\n",
    "    return adj_mat_corrupted, false_edges, false_non_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_train_corrupted, train_false_edges, train_false_non_edges = corrupt_adj(adj_train, 'link', percent=2)\n",
    "adj_val_corrupted, val_false_edges, val_false_non_edges = corrupt_adj(adj_validation, 'link', percent=2)\n",
    "adj_test_corrupted, test_false_edges, test_false_non_edges  = corrupt_adj(adj_test, 'link', percent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Supervised Learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neurons = 256\n",
    "input_rep = num_neurons + data.num_features\n",
    "\n",
    "class StructMLP(nn.Module):\n",
    "    def __init__(self, node_set_size=1):\n",
    "        super(StructMLP, self).__init__()\n",
    "\n",
    "        self.node_set_size = node_set_size\n",
    "        #Deepsets MLP\n",
    "\n",
    "        self.ds_layer_1 = nn.Linear(input_rep, num_neurons)\n",
    "        self.ds_layer_2 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.rho_layer_1 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.rho_layer_2 = nn.Linear(num_neurons, num_neurons)\n",
    "\n",
    "        #One Hidden Layer\n",
    "        self.layer1 = nn.Linear(num_neurons, num_neurons)\n",
    "        self.layer2 = nn.Linear(num_neurons, predictions[PREDICTION])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_tensor, samples):\n",
    "        #Deepsets initially on each of the samples\n",
    "        num_nodes = input_tensor.shape[1]\n",
    "        sum_tensor = torch.zeros(samples.shape[0], num_neurons).to(device)\n",
    "        for i in range(input_tensor.shape[0]):\n",
    "            #Process the input tensor to form n choose k combinations and create a zero tensor\n",
    "            set_init_rep = input_tensor[i].view(-1, input_rep)\n",
    "            x = self.ds_layer_1(set_init_rep)\n",
    "            x = self.relu(x)\n",
    "            x = self.ds_layer_2(x)\n",
    "            x = x[samples]\n",
    "            x = torch.sum(x, dim=1)\n",
    "            x = self.rho_layer_1(x)\n",
    "            sum_tensor += x\n",
    "\n",
    "        x = sum_tensor / input_tensor.shape[0]\n",
    "\n",
    "        #One Hidden Layer for predictor\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def compute_loss(self, input_tensor, samples, target):\n",
    "        pred = self.forward(input_tensor, samples)\n",
    "        return F.cross_entropy(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICTION == 'node':\n",
    "    node_set_size = 1\n",
    "elif PREDICTION == 'link':\n",
    "    node_set_size = 2\n",
    "else:\n",
    "    node_set_size = 3\n",
    "\n",
    "mlp = StructMLP(node_set_size).to(device)\n",
    "mlp_optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "mlp_model = 'best_mlp_model.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PREDICTION == 'node':\n",
    "    target_train = data.y[data.train_mask].type(torch.long)\n",
    "    target_val = data.y[data.val_mask].type(torch.long)\n",
    "    target_test = data.y[data.test_mask].type(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Supervised Learning Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_equal_number_edges_non_edges(adj_mat, false_non_edges, false_edges, small_samples):\n",
    "    edges = adj_mat.nonzero()\n",
    "    num_edges = edges.shape[0]\n",
    "    inverse_adj_mat = 1 - adj_mat\n",
    "    non_edges = inverse_adj_mat.nonzero()\n",
    "    num_non_edges  = non_edges.shape[0]\n",
    "    edges_sampled = edges[np.random.randint(num_edges, size=small_samples)]\n",
    "    non_edges_sampled = non_edges[np.random.randint(num_non_edges, size=small_samples)]\n",
    "    final_edges = []\n",
    "    final_non_edges = []\n",
    "    for ed in edges_sampled.tolist():\n",
    "        if ed not in false_edges:\n",
    "            final_edges.append(ed)\n",
    "    final_edges += false_non_edges\n",
    "    for n_ed in non_edges_sampled.tolist():\n",
    "        if n_ed not in false_non_edges:\n",
    "            final_non_edges.append(n_ed)\n",
    "    final_non_edges += false_edges\n",
    "\n",
    "    return final_edges, final_non_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  0.6930403709411621\n",
      "Validation Loss:  tensor(0.6932)\n",
      "Training Loss:  0.6898355484008789\n",
      "Validation Loss:  tensor(0.6921)\n",
      "Training Loss:  0.6852567195892334\n",
      "Validation Loss:  tensor(0.6897)\n",
      "Training Loss:  0.6771711707115173\n",
      "Validation Loss:  tensor(0.6870)\n",
      "Training Loss:  0.668122410774231\n",
      "Validation Loss:  tensor(0.6860)\n",
      "Training Loss:  0.6382704377174377\n",
      "Validation Loss:  tensor(0.6794)\n",
      "Training Loss:  0.6153746843338013\n",
      "Validation Loss:  tensor(0.6793)\n",
      "Training Loss:  0.6041514873504639\n",
      "Validation Loss:  tensor(0.6655)\n",
      "Training Loss:  0.5742831230163574\n",
      "Training Loss:  0.5762500762939453\n",
      "Training Loss:  0.571100115776062\n",
      "Training Loss:  0.5802671313285828\n",
      "Training Loss:  0.5920852422714233\n",
      "Training Loss:  0.5775364637374878\n",
      "Training Loss:  0.5819091200828552\n",
      "Validation Loss:  tensor(0.6198)\n",
      "Training Loss:  0.5255370140075684\n",
      "Training Loss:  0.49193063378334045\n",
      "Training Loss:  0.5111368298530579\n",
      "Training Loss:  0.5354408025741577\n",
      "Training Loss:  0.5448569059371948\n",
      "Training Loss:  0.5245049595832825\n",
      "Training Loss:  0.5232316255569458\n",
      "Training Loss:  0.4995958209037781\n",
      "Training Loss:  0.5165669918060303\n",
      "Training Loss:  0.478896826505661\n",
      "Training Loss:  0.5325345396995544\n",
      "Training Loss:  0.49342024326324463\n",
      "Training Loss:  0.5206747055053711\n",
      "Training Loss:  0.4948965311050415\n",
      "Training Loss:  0.49188145995140076\n",
      "Training Loss:  0.49576547741889954\n",
      "Training Loss:  0.5021288394927979\n",
      "Training Loss:  0.5315354466438293\n",
      "Training Loss:  0.47368374466896057\n",
      "Training Loss:  0.4763256311416626\n",
      "Training Loss:  0.492697536945343\n",
      "Training Loss:  0.4620245695114136\n",
      "Training Loss:  0.4684181213378906\n",
      "Training Loss:  0.44280320405960083\n",
      "Training Loss:  0.4638579189777374\n",
      "Training Loss:  0.5132961273193359\n",
      "Training Loss:  0.4818352162837982\n",
      "Training Loss:  0.4600621163845062\n",
      "Training Loss:  0.4730129539966583\n",
      "Training Loss:  0.48456963896751404\n",
      "Training Loss:  0.44453251361846924\n",
      "Training Loss:  0.465644508600235\n",
      "Training Loss:  0.4978930354118347\n",
      "Training Loss:  0.5102905631065369\n",
      "Training Loss:  0.5056502819061279\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "validation_loss = 10000.0\n",
    "small_samples = 200\n",
    "for num_epoch in range(epochs):\n",
    "    mlp_optimizer.zero_grad()\n",
    "    numbers = list(np.random.randint(500, size=NUM_SAMPLES))\n",
    "    hidden_samples_train = []\n",
    "    for number in numbers :\n",
    "        svd = TruncatedSVD(n_components=256, n_iter=10, random_state=number)\n",
    "        u_train = svd.fit_transform(adj_train_corrupted)\n",
    "        hidden_samples_train.append(torch.Tensor(u_train).to(device))\n",
    "    for i in range(NUM_SAMPLES):\n",
    "        hidden_samples_train[i] = torch.cat((hidden_samples_train[i].to(device), data.x[data.train_mask].to(device)),1)\n",
    "    input_ = torch.stack(hidden_samples_train)\n",
    "    input_ = input_.detach()\n",
    "    edges, non_edges = sample_equal_number_edges_non_edges(adj_train_corrupted, false_non_edges=train_false_non_edges, false_edges=train_false_edges, small_samples=200)\n",
    "    samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "    target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "    loss = mlp.compute_loss(input_, samples, target=target)\n",
    "    print(\"Training Loss: \", loss.item())\n",
    "    with torch.no_grad():\n",
    "        #Do Validation and check if validation loss has gone down\n",
    "        numbers = list(np.random.randint(500, size=NUM_SAMPLES))\n",
    "        hidden_samples_validation = []\n",
    "        for number in numbers :\n",
    "            svd = TruncatedSVD(n_components=256, n_iter=10, random_state=number)\n",
    "            u_validation = svd.fit_transform(adj_val_corrupted)\n",
    "            hidden_samples_validation.append(torch.Tensor(u_validation).to(device))\n",
    "        for i in range(NUM_SAMPLES):\n",
    "            hidden_samples_validation[i] = torch.cat((hidden_samples_validation[i].to(device), data.x[data.val_mask].to(device)),1)\n",
    "        input_val = torch.stack(hidden_samples_validation)\n",
    "        input_val = input_val.detach()\n",
    "        edges, non_edges = sample_equal_number_edges_non_edges(adj_val_corrupted, false_non_edges=val_false_non_edges, false_edges=val_false_edges, small_samples=200)\n",
    "        samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "        target_val = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "        compute_val_loss = mlp.compute_loss(input_val, samples, target=target_val)\n",
    "        if compute_val_loss < validation_loss:\n",
    "            validation_loss = compute_val_loss\n",
    "            print(\"Validation Loss: \", validation_loss)\n",
    "            #Save Model\n",
    "            torch.save(mlp.state_dict(), mlp_model)\n",
    "    loss.backward()\n",
    "    mlp_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = StructMLP(node_set_size).to(device)\n",
    "mlp.load_state_dict(torch.load(mlp_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass on the test graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = list(np.random.randint(500, size=NUM_SAMPLES))\n",
    "hidden_samples_test = []\n",
    "for number in numbers :\n",
    "    svd = TruncatedSVD(n_components=256, n_iter=10, random_state=number)\n",
    "    u_test = svd.fit_transform(adj_test_corrupted)\n",
    "    hidden_samples_test.append(torch.Tensor(u_test).to(device))\n",
    "for i in range(NUM_SAMPLES):\n",
    "    hidden_samples_test[i] = torch.cat((hidden_samples_test[i].to(device), data.x[data.test_mask].to(device)),1)\n",
    "\n",
    "    \n",
    "edges, non_edges = sample_equal_number_edges_non_edges(adj_test_corrupted, false_non_edges=test_false_non_edges, false_edges=test_false_edges, small_samples=200)\n",
    "samples = torch.cat((torch.Tensor(edges), torch.Tensor(non_edges)),dim=0).type(torch.long).to(device)\n",
    "target = torch.cat((torch.ones(len(edges)), torch.zeros(len(non_edges))),dim=0).type(torch.long).to(device)\n",
    "\n",
    "t_test = target.to(\"cpu\").numpy()\n",
    "input_test = torch.stack(hidden_samples_test)\n",
    "input_test = input_test.detach()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_pred = mlp.forward(input_test, samples)\n",
    "    pred = F.log_softmax(test_pred, dim=1)\n",
    "pred = pred.detach().to(\"cpu\").numpy()\n",
    "pred = np.argmax(pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Micro F1 Score:  0.5697399527186762\n",
      "Test Weighted F1 Score:  0.5495491361920902\n",
      "Test Accuracy Score:  0.5697399527186762\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Micro F1 Score: \", f1_score(t_test, pred, average='micro'))\n",
    "print(\"Test Weighted F1 Score: \", f1_score(t_test, pred, average='weighted'))\n",
    "print(\"Test Accuracy Score: \", accuracy_score(t_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
